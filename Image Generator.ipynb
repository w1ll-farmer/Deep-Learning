{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N22Uz-kLiZW"
   },
   "source": [
    "**Main imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MK1Jl7nkLnPA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as disp\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run1dh_hM0oO"
   },
   "source": [
    "**Import dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bK383zeDM4Ac",
    "outputId": "8a35453f-df4e-4cb6-c791-f88d713097c4"
   },
   "outputs": [],
   "source": [
    "# helper function to make getting another batch of data easier\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "class_names = ['apple','aquarium_fish','baby','bear','beaver','bed','bee','beetle','bicycle','bottle','bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle','chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup','dinosaur','dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard','lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain','mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree','plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket','rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider','squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor','train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # resize all images to 32x32\n",
    "    transforms.Resize((32, 32)),\n",
    "    # random flipping\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     # randomly rotate images up to 5 degrees\n",
    "#     transforms.RandomRotation(5),\n",
    "#     # affine transformations with shear\n",
    "#     transforms.RandomAffine(degrees=10, shear=5),\n",
    "#     # Gaussian blur\n",
    "#     transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
    "    # convert to tensor\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=train_transform),\n",
    "    batch_size=64, drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])),\n",
    "    batch_size=64, drop_last=True)\n",
    "\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
    "print(f'> Size of test dataset {len(test_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-FdW5HnimG2"
   },
   "source": [
    "**View some of the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "BtJs-qxHRLXz",
    "outputId": "90986b66-733e-41fc-d01f-ddc442c4423e"
   },
   "outputs": [],
   "source": [
    "# let's view some of the training data\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "x,t = next(train_iterator)\n",
    "x,t = x.to(device), t.to(device)\n",
    "plt.imshow(torchvision.utils.make_grid(x).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qnjh12UbNFpV"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim  \n",
    "        self.img_channels = 3  # RGB images\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.latent_dim, out_channels=96, kernel_size=4, stride=1, padding=0, bias=False),  # Output: 96x4x4\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=96, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 64x8x8\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=48, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 48x16x16\n",
    "            nn.BatchNorm2d(num_features=48),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=48, out_channels=self.img_channels, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 3x32x32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "    def sample(self, z):\n",
    "        with torch.no_grad():\n",
    "            samples = self.forward(z)\n",
    "        return samples\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_channels = 3  # RGB images\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.img_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 64x16x16\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 128x8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 256x4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),  # Output: 1x1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.model(img)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "\n",
    "# {'g_lr': 0.0006828838288502215, 'd_lr': 0.000275880375280674, 'step_size': 7, 'gamma': 0.1152882158531075} FID 44.55\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_channels = 3\n",
    "step_size = 7 \n",
    "gamma = 0.1152882158531075     # Reduce LR by multiplying by gamma\n",
    "\n",
    "# Initialize models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "# Optimisers and loss function\n",
    "optimiser_G = optim.Adam(generator.parameters(), lr=0.0006828838288502215, betas=(0.5, 0.999))\n",
    "optimiser_D = optim.Adam(discriminator.parameters(), lr=0.000275880375280674, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Schedulers\n",
    "scheduler_gen = StepLR(optimiser_G, step_size=step_size, gamma=gamma)\n",
    "scheduler_disc = StepLR(optimiser_D, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "print(f'> Number of generator parameters {len(torch.nn.utils.parameters_to_vector(generator.parameters()))}')\n",
    "print(f'> Number of discriminator parameters {len(torch.nn.utils.parameters_to_vector(discriminator.parameters()))}')\n",
    "total_params = len(torch.nn.utils.parameters_to_vector(generator.parameters())) + len(torch.nn.utils.parameters_to_vector(discriminator.parameters()))\n",
    "print(total_params)\n",
    "if total_params > 1000000:\n",
    "    print(\"> Warning: you have gone over your parameter budget and will have a grade penalty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "batch_size = 64\n",
    "epochs = 0\n",
    "\n",
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'latent_dim': latent_dim,\n",
    "    'n_channels': img_channels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while steps < 50000:\n",
    "    loss_arr_D = np.zeros(0)\n",
    "    loss_arr_G = np.zeros(0)\n",
    "\n",
    "    for _ in range(1000):\n",
    "        x, _ = next(train_iterator)\n",
    "\n",
    "        real_imgs = x.cuda()\n",
    "\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        real_labels = torch.ones(batch_size).to(device)\n",
    "        fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "        # Real images loss\n",
    "        real_loss = criterion(discriminator(real_imgs), real_labels)\n",
    "        # Fake images loss\n",
    "        # Make sure the output shape is [batch_size, 1] (scalar per image)\n",
    "        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        optimiser_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimiser_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
    "\n",
    "        optimiser_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimiser_G.step()\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        loss_arr_D = np.append(loss_arr_D, d_loss.item())\n",
    "        loss_arr_G = np.append(loss_arr_G, g_loss.item())\n",
    "\n",
    "        if steps >= 50000:\n",
    "            break\n",
    "\n",
    "    print(f\"Steps: {steps}, Loss D: {loss_arr_D.mean():.3f}, Loss G: {loss_arr_G.mean():.3f}\")\n",
    "\n",
    "    # Sample model and visualize results\n",
    "    generator.eval()\n",
    "    z = torch.randn(64, latent_dim, 1, 1).cuda()\n",
    "    samples = generator.sample(z).cpu().detach()\n",
    "    plt.imshow(torchvision.utils.make_grid(samples).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "    plt.show()\n",
    "    disp.clear_output(wait=True)\n",
    "    epochs += 1\n",
    "    generator.train()\n",
    "    if epochs % step_size == 0:\n",
    "        scheduler_gen.step()\n",
    "        scheduler_disc.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew-Ik2p6pIkW"
   },
   "source": [
    "**Latent interpolations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "Sbb3a--HkGzZ",
    "outputId": "f8d8cbcd-f052-48a7-997e-9b44f42695ab"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "# Assuming 'batch_size' and 'latent_dim' are defined\n",
    "\n",
    "# Generate two random latent vectors\n",
    "z0 = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "z7 = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "\n",
    "# Prepare the grid for storing interpolated images\n",
    "grid = []\n",
    "for i in range(8):\n",
    "    a = i / 7  \n",
    "    b = 1 - a\n",
    "    z_terp = (b * z0) + (a * z7) \n",
    "\n",
    "    row = generator.sample(z_terp).detach().cpu()[:8]  \n",
    "    \n",
    "    grid.append(row)\n",
    "\n",
    "# Concatenate the rows to form the grid\n",
    "grid = torch.cat(grid, dim=0)\n",
    "\n",
    "# Display the 8x8 grid of interpolated images\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.grid(False)\n",
    "plt.imshow(torchvision.utils.make_grid(grid, normalize=True).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "\n",
    "\n",
    "z0 = torch.randn(batch_size, latent_dim, 1, 1,device=device)\n",
    "z7 = torch.randn(batch_size, latent_dim, 1, 1,device=device)\n",
    "\n",
    "grid = []\n",
    "for i in range(8):\n",
    "    a = i / 7\n",
    "    b = 1 - a\n",
    "    z_terp = (b*z0) + (a*z7)\n",
    "#     print(z_terp)\n",
    "    row = generator.sample(z_terp).detach().cpu()[:8]\n",
    "    \n",
    "#     row = (row+1)/2\n",
    "    grid.append(row)\n",
    "grid = torch.concat(grid, dim=0)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.grid(False)\n",
    "plt.imshow(torchvision.utils.make_grid(grid).cpu().numpy().transpose(1,2,0), cmap=plt.cm.binary)\n",
    "# plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1Vxe_qIbRXS"
   },
   "source": [
    "**FID scores**\n",
    "\n",
    "Evaluate the FID from 10k of your model samples (do not sample more than this) and compare it against the 10k test images. Calculating FID is somewhat involved, so we use a library for it. It can take a few minutes to evaluate. Lower FID scores are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RmRO6U7mLbq"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install clean-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "4RmRO6U7mLbq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from cleanfid import fid\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "canqHlS7bRXT"
   },
   "outputs": [],
   "source": [
    "# define directories\n",
    "real_images_dir = 'real_images'\n",
    "generated_images_dir = 'generated_images'\n",
    "num_samples = 10000 # do not change\n",
    "\n",
    "# create/clean the directories\n",
    "def setup_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        !rm -r {directory} # remove any existing (old) data\n",
    "    os.makedirs(directory)\n",
    "\n",
    "setup_directory(real_images_dir)\n",
    "setup_directory(generated_images_dir)\n",
    "\n",
    "# generate and save 10k model samples\n",
    "num_generated = 0\n",
    "while num_generated < num_samples:\n",
    "\n",
    "    # sample from your model, you can modify this\n",
    "    z = torch.randn(64, latent_dim, 1, 1).cuda()\n",
    "    samples_batch = generator.sample(z).cpu().detach()\n",
    "\n",
    "    for image in samples_batch:\n",
    "        if num_generated >= num_samples:\n",
    "            break\n",
    "        save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
    "        num_generated += 1\n",
    "\n",
    "# save 10k images from the CIFAR-100 test dataset\n",
    "num_saved_real = 0\n",
    "while num_saved_real < num_samples:\n",
    "    real_samples_batch, _ = next(test_iterator)\n",
    "    for image in real_samples_batch:\n",
    "        if num_saved_real >= num_samples:\n",
    "            break\n",
    "        save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
    "        num_saved_real += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RVOmtzabRXT",
    "outputId": "33d56f24-a300-4a81-e8f7-cfaedc180411"
   },
   "outputs": [],
   "source": [
    "# compute FID\n",
    "score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
    "print(f\"FID score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "lpips_model = lpips.LPIPS(net='vgg')  \n",
    "lpips_model.eval() \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lpips_model.to(device)\n",
    "\n",
    "# New train dataset transformations so that it aligns with how lpips model was trained\n",
    "lpips_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=lpips_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Sample a batch of 64 images\n",
    "sample_batch, _ = next(iter(train_loader))  # Get a batch from the train_loader\n",
    "sample_batch = sample_batch.to(device)  # Move the batch to the appropriate device\n",
    "\n",
    "\n",
    "lpips_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample_img in enumerate(sample_batch):\n",
    "        sample_img = sample_img.unsqueeze(0)  # Add batch dimension for LPIPS model\n",
    "\n",
    "       \n",
    "        for img, label in train_loader:\n",
    "            img = img.to(device)\n",
    "            dist = lpips_model(sample_img, img)  # Compute LPIPS score between images\n",
    "\n",
    "            \n",
    "            for d in dist.squeeze(0):  \n",
    "                lpips_scores.append(d.item()) \n",
    "\n",
    "# Convert list of LPIPS scores to a tensor\n",
    "lpips_scores = torch.tensor(lpips_scores)\n",
    "# Output mean and standard deviation of lpips scores to identify how similar images are to nearest neighbours\n",
    "print(np.array(lpips_scores).mean(), np.array(lpips_scores).std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DCGAN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna \n",
    "import os\n",
    "from cleanfid import fid\n",
    "from torchvision.utils import save_image\n",
    "img_channels = 3\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim  \n",
    "        self.img_channels = 3  # RGB images\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.latent_dim, out_channels=96, kernel_size=4, stride=1, padding=0, bias=False),  # Output: 96x4x4\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=96, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 64x8x8\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=48, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 48x16x16\n",
    "            nn.BatchNorm2d(num_features=48),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=48, out_channels=self.img_channels, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 3x32x32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "    def sample(self, z):\n",
    "        with torch.no_grad():\n",
    "            samples = self.forward(z)\n",
    "        return samples\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_channels = 3  # RGB images\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.img_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 64x16x16\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 128x8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),  # Output: 256x4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0, bias=False),  # Output: 1x1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.model(img)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    g_lr = trial.suggest_loguniform(\"g_lr\", 2.5e-4, 9.5e-4)\n",
    "    d_lr = trial.suggest_loguniform(\"d_lr\", 2.5e-4, 9.5e-4)\n",
    "#     latent_dim = trial.suggest_int(\"latent_dim\", 64, 128, step=32)\n",
    "    step_size = trial.suggest_int(\"step_size\", 5, 20)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.1, 0.85)\n",
    "#     dropout = trial.suggest_float(\"dropout\",0.12,0.25)\n",
    "#     b1 = trial.suggest_float(\"b1\", 0.475, 0.525)\n",
    "#     b2 = trial.suggest_float(\"b2\", 0.96, 0.9999)\n",
    "    b1 = 0.5\n",
    "    b2 = 0.99\n",
    "#     print(f\"g_lr: {g_lr}, d_lr: {d_lr}, latent_dim: {latent_dim}\")\n",
    "#     print(f\"step_size: {step_size_g, step_size_d}, gamma_g: {gamma_g}, gamma_d: {gamma_d})\")\n",
    "    \n",
    "\n",
    "    latent_dim = 100\n",
    "#     - {'g_lr': 0.0006828838288502215, 'd_lr': 0.000275880375280674, 'step_size': 7, 'gamma': 0.1152882158531075} FID 44.55\n",
    "    \n",
    "    generator = Generator(latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    # Optimisers and scheduler\n",
    "    optimiser_G = optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2))\n",
    "    optimiser_D = optim.Adam(discriminator.parameters(), lr=d_lr, betas=(b1, b2))\n",
    "    scheduler_gen = torch.optim.lr_scheduler.StepLR(optimiser_G, step_size=step_size, gamma=gamma)\n",
    "    scheduler_disc = torch.optim.lr_scheduler.StepLR(optimiser_D, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    steps = 0\n",
    "    epochs = 0\n",
    "    batch_size = 64\n",
    "    params = {\n",
    "        'batch_size': 64,\n",
    "        'latent_dim': latent_dim,\n",
    "        'n_channels': img_channels\n",
    "    }\n",
    "    \n",
    "    while steps < 50000:\n",
    "        loss_arr_D = np.zeros(0)\n",
    "        loss_arr_G = np.zeros(0)\n",
    "\n",
    "        for _ in range(1000):\n",
    "            x, _ = next(train_iterator)\n",
    "\n",
    "            real_imgs = x.cuda()\n",
    "            # Train Discriminator\n",
    "            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "\n",
    "            real_labels = torch.ones(batch_size).to(device)\n",
    "            fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "            # Real images loss\n",
    "            real_loss = criterion(discriminator(real_imgs), real_labels)\n",
    "            # Fake images loss\n",
    "            fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)\n",
    "            d_loss = real_loss + fake_loss\n",
    "\n",
    "            optimiser_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimiser_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
    "\n",
    "            optimiser_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimiser_G.step()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            loss_arr_D = np.append(loss_arr_D, d_loss.item())\n",
    "            loss_arr_G = np.append(loss_arr_G, g_loss.item())\n",
    "\n",
    "            if steps >= 50000:\n",
    "                break\n",
    "\n",
    "        print(f\"Steps: {steps}, Loss D: {loss_arr_D.mean():.3f}, Loss G: {loss_arr_G.mean():.3f}\")\n",
    "\n",
    "        # Sample model and visualize results\n",
    "        generator.eval()\n",
    "        z = torch.randn(64, latent_dim, 1, 1).cuda()\n",
    "        samples = generator.sample(z).cpu().detach()\n",
    "        plt.imshow(torchvision.utils.make_grid(samples).cpu().numpy().transpose(1, 2, 0), cmap=plt.cm.binary)\n",
    "        plt.show()\n",
    "        epochs += 1\n",
    "        generator.train()\n",
    "        if epochs % step_size == 0:\n",
    "            scheduler_gen.step()\n",
    "            scheduler_disc.step()\n",
    "            \n",
    "    \n",
    "    # define directories\n",
    "    real_images_dir = 'real_images'\n",
    "    generated_images_dir = 'generated_images'\n",
    "    num_samples = 10000 # do not change\n",
    "\n",
    "    # create/clean the directories\n",
    "    def setup_directory(directory):\n",
    "        if os.path.exists(directory):\n",
    "            !rm -r {directory} # remove any existing (old) data\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    setup_directory(real_images_dir)\n",
    "    setup_directory(generated_images_dir)\n",
    "\n",
    "    # generate and save 10k model samples\n",
    "    num_generated = 0\n",
    "    while num_generated < num_samples:\n",
    "\n",
    "        # sample from your model, you can modify this\n",
    "        z = torch.randn(64, latent_dim, 1, 1).cuda()\n",
    "        samples_batch = generator.sample(z).cpu().detach()\n",
    "\n",
    "        for image in samples_batch:\n",
    "            if num_generated >= num_samples:\n",
    "                break\n",
    "            save_image(image, os.path.join(generated_images_dir, f\"gen_img_{num_generated}.png\"))\n",
    "            num_generated += 1\n",
    "\n",
    "    # save 10k images from the CIFAR-100 test dataset\n",
    "    num_saved_real = 0\n",
    "    while num_saved_real < num_samples:\n",
    "        real_samples_batch, _ = next(test_iterator)\n",
    "        for image in real_samples_batch:\n",
    "            if num_saved_real >= num_samples:\n",
    "                break\n",
    "            save_image(image, os.path.join(real_images_dir, f\"real_img_{num_saved_real}.png\"))\n",
    "            num_saved_real += 1\n",
    "    \n",
    "    # compute FID\n",
    "    score = fid.compute_fid(real_images_dir, generated_images_dir, mode=\"clean\")\n",
    "    print(f\"FID score: {score}\")\n",
    "    file = open(\"FID-Scores\",\"a\")\n",
    "    file.write(f\"{score}, ({b1},{b2}), g_lr: {g_lr}, d_lr: {d_lr}, latent_dim: {latent_dim}, step_size: {step_size}, gamma: {gamma}\\n\")\n",
    "    file.close()\n",
    "    return score\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Param count: 598148\n",
    " - Base DCGAN: FID 313\n",
    " - WGAN with feature matching and gradient penalty: FID 282\n",
    " - WGAN + fmgp + all train transformations: FID 324\n",
    " - WGAN + gfmp + reduced transformations + betas .9-.999: FID 272\n",
    " - above + d-lr=0.0002, g-lr=0.0001 (prev other way round): FID 217\n",
    " - without shear/rotations + above: FID 219\n",
    " \n",
    "Param Count: 595776\n",
    " - DCGAN with g_lr=0.0001 and d_lr==0.0002: FID 422?\n",
    " - DCGAN with StepLR both lrs equal: FID 111\n",
    " - {'g_lr': 0.0006814031697767739, 'd_lr': 0.00012139497245059714, 'latent_dim': 64, 'step_size': 9, 'gamma': 0.6555892021167088, 'b1': 0.49814188804932835, 'b2': 0.9890717565690195}: FID 95.94\n",
    " - {'g_lr': 0.0006400197806639131, 'd_lr': 0.0004680754143284617, 'latent_dim': 224, 'step_size': 9, 'gamma': 0.703364065823086, 'b1': 0.5120438113537247, 'b2': 0.9928494626639106} FID: 95.13\n",
    " - {'g_lr': 0.0009286904414305756, 'd_lr': 0.00040507457935635836, 'latent_dim': 160, 'step_size': 9, 'gamma': 0.28468433714087593, 'b1': 0.4815824307625205, 'b2': 0.9587758651885724} - FID: 97.3\n",
    " - {'g_lr': 0.00036455004741175876, 'd_lr': 0.000739064082810268, 'latent_dim': 160, 'step_size': 10, 'gamma': 0.49525170974763577, 'b1': 0.4984104132166096, 'b2': 0.9996938161836509} FID: 97.25\n",
    " - {'g_lr': 0.0006305403183965492, 'd_lr': 0.00024562829361544024, 'latent_dim': 64, 'step_size': 6, 'gamma': 0.8982742062774087, 'b1': 0.512029309540369, 'b2': 0.9853474704544104} FID: 91.94\n",
    "\n",
    "Param Count: 891968\n",
    "\n",
    "- {'g_lr': 0.00037579454545191366, 'd_lr': 0.00013508523644514142, 'latent_dim': 64, 'step_size': 13, 'gamma': 0.32895807737962646} FID: 83.36\n",
    "- {'g_lr': 0.0003813418582686221, 'd_lr': 0.00028307216565527947, 'latent_dim': 128, 'step_size': 10, 'gamma': 0.6912483540642389} FID: 91.43\n",
    "- {'g_lr': 0.0008713135600597669, 'd_lr': 0.0002442567087266487, 'latent_dim': 192, 'step_size': 7, 'gamma': 0.4740671338389559} FID: 86.90\n",
    "- {'g_lr': 0.00036916284893116796, 'd_lr': 0.000603707872600647, 'latent_dim': 64, 'step_size': 12, 'gamma': 0.7924788560314879}. FID: 91.06\n",
    "- {'g_lr': 0.00036916284893116796, 'd_lr': 0.000603707872600647, 'latent_dim': 64, 'step_size': 12, 'gamma': 0.7924788560314879}. FID: 91.06\n",
    "- {'g_lr': 0.0008505944870268719, 'd_lr': 0.00013707522718043292, 'latent_dim': 192, 'step_size': 12, 'gamma': 0.5506931925857736} FID: 83.92\n",
    "- {'g_lr': 0.0006136210845775221, 'd_lr': 0.0002139380113632377, 'latent_dim': 96, 'step_size': 7, 'gamma': 0.7072523890037579} FID 86.44\n",
    "\n",
    "discriminator lr <= g_lr\n",
    " \n",
    " - {'g_lr': 0.0007340907919889216, 'd_lr': 0.0001927856548861613, 'latent_dim': 128, 'step_size': 19, 'gamma': 0.5373091857650582}. Best is trial 0 with value: 85.10373456772481. FID 85.1\n",
    " - {'g_lr': 0.0005145033935415546, 'd_lr': 0.00010229425253813181, 'latent_dim': 128, 'step_size': 20, 'gamma': 0.8714934386353768} FID 87.4\n",
    " - {'g_lr': 0.0009417594819548194, 'd_lr': 0.00027712121409213965, 'latent_dim': 64, 'step_size': 7, 'gamma': 0.2705894174461227} FID: 86.28\n",
    " - {'g_lr': 0.0006511206888448697, 'd_lr': 0.0002183311745326889, 'latent_dim': 64, 'step_size': 9, 'gamma': 0.735978442880421} FID 87.18\n",
    "\n",
    "- {'g_lr': 0.00043838182732142735, 'd_lr': 0.00016770962354068362, 'latent_dim': 64, 'step_size': 14, 'gamma': 0.7588884139473471} FID 88.98\n",
    "- {'dropout': 0.24974735461990838}: 85.74\n",
    "- {'dropout': 0.13642056472432817}: 86.73\n",
    "- {'dropout': 0.30276459406217154}: 88.85\n",
    "- {'dropout': 0.2518147586184857}. 89.61\n",
    "- {'dropout': 0.13309362888176604} 84.66\n",
    " - 0.22450924820171564: 85.55\n",
    "\n",
    "**967,072 params**\n",
    "- transforms + tuned learning rates and latent_dim: FID 98.29\n",
    "- transforms + 0.0002 lr + 100 latent_dim: FID 84.88\n",
    "- Resize only + above: FID 58.77!! -> 56.48\n",
    "- {'g_lr': 0.00047011897412151387, 'd_lr': 0.00032888835823867305, 'step_size': 14, 'gamma': 0.13605607382582346} FID 52.98736270293523\n",
    "- {'g_lr': 0.0004646569322993303, 'd_lr': 0.0006323606861306212, 'step_size': 5, 'gamma': 0.8797304441483801} FID 51.49\n",
    "- {'g_lr': 0.0003742610055813428, 'd_lr': 0.0004009095378355368, 'step_size': 5, 'gamma': 0.8910603576191873} FID 51.59 low step size + high gamma or vice versa works quite well\n",
    "- {'g_lr': 0.00024730537498181265, 'd_lr': 0.0004064663394286604, 'step_size': 12, 'gamma': 0.14786319216080435} FID 54.92\n",
    "- {'g_lr': 0.00010054020620311011, 'd_lr': 0.00010364349860835665, 'step_size': 9, 'gamma': 0.19138821661905886} FID 66.02, low base LRs + low gamma and step size all combine badly\n",
    "- {'g_lr': 0.0005904097365207748, 'd_lr': 0.0009556906871456465, 'step_size': 19, 'gamma': 0.3100246769446714} fid 50.47\n",
    "- {'g_lr': 0.0005502189910110365, 'd_lr': 0.00012141854619756989, 'step_size': 5, 'gamma': 0.29843701854660687} FID 49.16\n",
    "- {'g_lr': 0.0005132927362642562, 'd_lr': 0.00012275457879686526, 'step_size': 10, 'gamma': 0.5822640412702591} FID 48.65\n",
    "- {'g_lr': 0.00039890686782809696, 'd_lr': 0.0007170630663625129, 'step_size': 14, 'gamma': 0.10723648399528232} FID 51.29\n",
    "- {'g_lr': 0.0005749455818234153, 'd_lr': 0.0002958197122217904, 'step_size': 9, 'gamma': 0.40123837184511213} fid 48.63\n",
    "- {'g_lr': 0.0004465779109723407, 'd_lr': 0.0008005048604263282, 'step_size': 10, 'gamma': 0.15622517275303516} FID 49.91\n",
    "- {'g_lr': 0.0004301786361677816, 'd_lr': 0.0004733102504837982, 'step_size': 11, 'gamma': 0.8248953417036737} FID 49.85\n",
    " - {'g_lr': 0.00029525225977134736, 'd_lr': 0.000703981679873739, 'step_size': 10, 'gamma': 0.6751003500761057} low fid, likely due to huge gap betwen d_lr and g_lr\n",
    "  - {'g_lr': 0.0006006667193773175, 'd_lr': 0.0005898293581241078, 'step_size': 6, 'gamma': 0.2797635525687869 FID 45.62\n",
    "\n",
    "- {‘g_lr': 0.0006138031334792676, 'd_lr': 0.0005625836234903569, 'step_size': 5, 'gamma': 0.3300497180274462} FID47.77\n",
    "\n",
    "- {‘g_lr': 0.0006573019293906829, 'd_lr': 0.0006111283690657075, 'step_size': 5, 'gamma': 0.3104907558908947} FID48.45\n",
    " - {'g_lr': 0.0007508166505062501, 'd_lr': 0.000453966691674159, 'step_size': 6, 'gamma': 0.20783980641627642} FID 44.46\n",
    " - {'g_lr': 0.0005925841689063689, 'd_lr': 0.0003828441036673148, 'step_size': 7, 'gamma': 0.4680577304310266} FID 46.29\n",
    "  - {'g_lr': 0.0006828838288502215, 'd_lr': 0.000275880375280674, 'step_size': 7, 'gamma': 0.1152882158531075} FID 44.55\n",
    " - {'g_lr': 0.0007707339926668834, 'd_lr': 0.00027338506365327066, 'step_size': 15, 'gamma': 0.1037305611657213} fid 47.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearningkernel",
   "language": "python",
   "name": "deeplearningkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
